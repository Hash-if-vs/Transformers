{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZE QUERY KEY AND VALUE VECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L,d_k, d_v=4,8,8\n",
    "q=np.random.randn(L,d_k)\n",
    "k=np.random.randn(L,d_k)\n",
    "v=np.random.randn(L ,d_v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.80455444, -2.7930426 , -0.88015216,  2.40713596],\n",
       "       [ 7.527268  , -2.89781127,  1.91688114,  2.02065071],\n",
       "       [-5.81449877,  5.50958336, -2.52298173, -0.29200241],\n",
       "       [ 1.67064351,  4.71477264, -1.2562126 , -0.92202523]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_scaled= np.dot(q,k.T)\n",
    "non_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8963136066040833, 1.0250371090854091, 11.901918793723505)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Variances \n",
    "q.var(),k.var(),non_scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8963136066040833, 1.0250371090854091, 1.487739849215438)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it can be seen that the variance of q.kT is very high, we need to reduce it by scaling\n",
    "scaled= np.dot(q,k.T)/math.sqrt(d_k)\n",
    "q.var(),k.var(),scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.34511312, -0.98748968, -0.31118078,  0.85105108],\n",
       "       [ 2.66129112, -1.024531  ,  0.67771983,  0.71440791],\n",
       "       [-2.05573576,  1.94793188, -0.89200875, -0.10323844],\n",
       "       [ 0.59066168,  1.66692385, -0.44413823, -0.32598515]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled\n",
    "#we can see that the values were reduced to a range and of low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "- This is to ensure words don't get context from words generated in the future. \n",
    "- Not required in the encoders, but required int he decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask =np.tril(np.ones((L,L)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask==0]=-np.infty\n",
    "mask[mask==0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.34511312,        -inf,        -inf,        -inf],\n",
       "       [ 3.66129112, -0.024531  ,        -inf,        -inf],\n",
       "       [-1.05573576,  2.94793188,  0.10799125,        -inf],\n",
       "       [ 1.59066168,  2.66692385,  0.55586177,  0.67401485]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled+mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "$$\n",
    "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.9755369 , 0.0244631 , 0.        , 0.        ],\n",
       "       [0.01694898, 0.92878303, 0.05426799, 0.        ],\n",
       "       [0.21327198, 0.6256746 , 0.07577499, 0.08527842]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention= softmax(scaled+mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.10126308, -0.04994959,  1.02084367,  0.17022373, -1.8905071 ,\n",
       "         -0.70856559, -1.75184522,  0.0985254 ],\n",
       "        [ 1.05659271, -0.05169121,  0.98991741,  0.19217061, -1.85490828,\n",
       "         -0.65186021, -1.6880424 ,  0.13115717],\n",
       "        [-0.79149402, -0.1951564 , -0.22648053,  0.9948521 , -0.39918437,\n",
       "          1.43942004,  0.7767421 ,  1.19898806],\n",
       "        [-0.46934977, -0.14927797,  0.10617265,  0.64403253, -0.58614919,\n",
       "          0.9242513 ,  0.15725369,  0.80228357]]),\n",
       " array([[ 1.10126308, -0.04994959,  1.02084367,  0.17022373, -1.8905071 ,\n",
       "         -0.70856559, -1.75184522,  0.0985254 ],\n",
       "        [-0.72476787, -0.12114315, -0.24335655,  1.06736605, -0.43530231,\n",
       "          1.60943067,  0.85627943,  1.43244349],\n",
       "        [-2.52464119, -1.50722554, -0.32721612,  0.01134205,  0.68473506,\n",
       "         -0.79941043,  0.20520903, -2.45284664],\n",
       "        [-0.69706738,  0.60250978,  0.76821441, -0.71477277,  0.43990983,\n",
       "          1.51226901, -0.23955433,  0.83129813]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v=np.dot(attention,v)\n",
    "new_v,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    scaled = scaled + mask\n",
    "  attention = softmax(scaled)\n",
    "  out = np.matmul(attention, v)\n",
    "  return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-0.53956003 -0.7278342   0.79487738 -0.46728178 -0.52146064  0.45303998\n",
      "   1.91997477 -0.73071626]\n",
      " [ 0.3558573   0.7139521   0.95890709  1.01977779 -0.67939045  1.89435826\n",
      "   0.96634299  0.52971517]\n",
      " [-1.03448014 -0.0193485   0.63200876 -0.24783897 -0.54612951 -1.7305552\n",
      "  -1.33485169  0.34452031]\n",
      " [-1.69628504  0.16801981  1.16609652  1.26262055  0.13514221  0.00291674\n",
      "  -0.4809035  -1.2920545 ]]\n",
      "K\n",
      " [[-0.62326288  0.48682762 -0.53659623  0.65433147 -1.49399641  3.01245882\n",
      "   0.96469085 -0.76483198]\n",
      " [-2.04244406  0.18643837 -0.69058378  0.70425149 -1.13689369 -0.65008176\n",
      "  -1.78358513 -0.33494799]\n",
      " [ 0.82066678  1.21676776 -0.01005504  0.23899918  0.86422993  0.26624263\n",
      "   0.53252604  0.17033795]\n",
      " [ 0.49164881 -0.76981682  0.10685198  0.61704574 -1.75762392 -0.23120006\n",
      "   0.84706998  0.16782274]]\n",
      "V\n",
      " [[ 1.10126308 -0.04994959  1.02084367  0.17022373 -1.8905071  -0.70856559\n",
      "  -1.75184522  0.0985254 ]\n",
      " [-0.72476787 -0.12114315 -0.24335655  1.06736605 -0.43530231  1.60943067\n",
      "   0.85627943  1.43244349]\n",
      " [-2.52464119 -1.50722554 -0.32721612  0.01134205  0.68473506 -0.79941043\n",
      "   0.20520903 -2.45284664]\n",
      " [-0.69706738  0.60250978  0.76821441 -0.71477277  0.43990983  1.51226901\n",
      "  -0.23955433  0.83129813]]\n",
      "New V\n",
      " [[ 1.10126308 -0.04994959  1.02084367  0.17022373 -1.8905071  -0.70856559\n",
      "  -1.75184522  0.0985254 ]\n",
      " [ 1.05659271 -0.05169121  0.98991741  0.19217061 -1.85490828 -0.65186021\n",
      "  -1.6880424   0.13115717]\n",
      " [-0.79149402 -0.1951564  -0.22648053  0.9948521  -0.39918437  1.43942004\n",
      "   0.7767421   1.19898806]\n",
      " [-0.46934977 -0.14927797  0.10617265  0.64403253 -0.58614919  0.9242513\n",
      "   0.15725369  0.80228357]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.9755369  0.0244631  0.         0.        ]\n",
      " [0.01694898 0.92878303 0.05426799 0.        ]\n",
      " [0.21327198 0.6256746  0.07577499 0.08527842]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "materials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
